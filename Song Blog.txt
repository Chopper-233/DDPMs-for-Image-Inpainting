likelihood based - intractable normalising constants
implicit models - adversarial training is unstable

score based - model the score function = gradient of the log probability density function
 		- learnt using score matching - minimisation of the fisher divergence without knowledge of the ground truth data score
		- fisher divergence doesnt require a normalised dist
		- we can represent a distribution by modeling its score function, which can be estimated by training a score-based model of free-form architectures with score matching.

langevin dynamics - iterative procedure to draw samples from trained score based model

mcmc - a class of algorithms for sampling from a probability distribution

naive approach - estimated score functions are innaccurate in low density regions
		   - expected as score matching minimizes the Fisher divergence
		   - problematic as initial sample will likely be in low density region

solution - add noise to training data
	   - train score based models on noisy data
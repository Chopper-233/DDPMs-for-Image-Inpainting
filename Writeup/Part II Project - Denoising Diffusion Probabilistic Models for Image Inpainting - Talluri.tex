\documentclass{article}

\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{array,booktabs,enumitem}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[backend=biber, style=numeric, sorting=ynt]{biblatex}
\addbibresource{ref.bib}

\title{Denoising Diffusion Probabilistic Models for Image Inpainting}
\author{Pranav Talluri}
\date{2022/23}

\begin{document}


\maketitle
\newpage
\section*{Declaration of Originality}
\newpage
\section*{Proforma}
\newpage
\tableofcontents
\newpage
\section{Introduction}

TODO

\subsection{Motivation}

Deep generative models "enabled scalable modeling of complex, high-dimensional data including images, text, and speech" - https://deepgenerativemodels.github.io/

The idea that computers could generate novel, creative works 

"GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature. VAE relies on a surrogate loss. Flow models have to use specialized architectures to construct reversible transform." - https://lilianweng.github.io/posts/2021-07-11-diffusion-models/

DPMs - parameterised markov chains trained to denoise data. they are inspired by non-equilibrium statistical physics, where 

DDPMs - DPMs trained on a weighted variational bound designed according to a connection between DPMs and denoising score matching with Langevin Dynamics

DDPMs offer a solution\dots

Applications of DDPMs\dots

Image inpainting\dots

\subsection{Previous Works}

\newpage
\section{Preparation}

PrepWork\dots

\begin{itemize}
    \item logistic distribution
    \begin{itemize}
        \item normal dist but with heavy tails 
        \item "increases the robustness of analyses based on it compared with using the normal distribution"
    \end{itemize}
    \item PixelCNN++
    \begin{itemize}
        \item openai implementation of PixelCNN
        \item tractable likelihood
        \item "model fully factorizes the probability density function on an image x over all its sub-pixels"
        \item modification to PixelCNN - discretised logistic mixture likelihood rather than softmax
        \item modification - conditions on whole pixels rather than rgb vals
        \item modification - downsampling to encourage long range dependencies
        \item modification - shortcut connections
        \item modification - standard binary dropout to prevent overfitting
    \end{itemize}
    \item DDPMs (modified PixelCNN++)
    \begin{itemize}
        \item replaced weight normalisation to group normalisation - simplicity 
        \item 4 resolution levels for 32x32 and 6 for 256x256
        \item two convolutional residual blocks per resolution level
        \item "self-attention blocks at the 16x16 resolution between the convolutional blocks"
        \item dropout rate set by sweeping over values
        \item linear beta schedule
        \item random horizontal flips
        \item Adam rather than RMSProp
        \item batch size is 128 for CIFAR
        \item EMA set to 0.9999 decay factor
    \end{itemize}
    \item ResNets
    \begin{itemize}
        \item 
    \end{itemize}
\end{itemize}

Related works\dots

Maths behind DDPMs\dots

Alternative methods of image generation\dots

Alternative methods of image inpainting\dots


\newpage
\section{Implementation}
\newpage
\section{Evaluation}
\newpage
\section{Conclusion}
\newpage
\printbibliography
\newpage
\section*{Appendicies}
\newpage
\section*{Index}
\newpage


\end{document}